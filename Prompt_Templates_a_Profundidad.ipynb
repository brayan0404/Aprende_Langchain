{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgAJn6hJZIxWgrxMRJ2Ff1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brayan0404/Aprende_Langchain/blob/main/Prompt_Templates_a_Profundidad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ISTALATIONS, IMPORTS AND API SETUP SECTION**"
      ],
      "metadata": {
        "id": "Kx-VtOZjVIXC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MznR_SUZ2GFF"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "OGbIl-XgQotI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"coloca_tu_open_ai_api_key_aqui\""
      ],
      "metadata": {
        "id": "yni_mNTfR10E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM SECTION**"
      ],
      "metadata": {
        "id": "Sq6q0wO0U52s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(\n",
        "    openai_api_key=\"OPENAI_API_KEY\",\n",
        "    model_name = \"text-davinci-003\",\n",
        "    )"
      ],
      "metadata": {
        "id": "NhqVeOemR-XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui podemos observar que ademas de establecer la conexion con openai, tenemos una variable nombrada **model_name**, cuyo valor asociado es **text-davinci-003**. El anterior corresponde al nombre del llm que vamos a estar utilizando. Cuando creamos una aplicacion en langchain y no establecemos el nombre del modelo este será asignado por defecto, de echo, el asignado sera el que aqui contenemos. Se considera buena practica colocar el modelo."
      ],
      "metadata": {
        "id": "aQJWDYsxfeN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.3)"
      ],
      "metadata": {
        "id": "Bm2z27y8UNtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se establece una temperature no tan alta para que el modelo se riga sobre la informacion que le suministramos y no divague, y no tan baja para que tenga la capacidad de hacer deduciones sobre el texto cuando se le pida."
      ],
      "metadata": {
        "id": "D9X9cduvgmj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PROMPT-TEMPLATE SECTION**"
      ],
      "metadata": {
        "id": "4Ka0B2QAVcFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        " Basado en el Contexto que te proporcione responde la pregunta o realiza la solicitud que te hago.\n",
        " Si no sabes la repuesta a la pregunta responde \"Disculpa, no se la respuesta\".\n",
        " La unica informacion de la que podras extraer o crear la respuesta es de la informacion que te\n",
        " proporciono en el Contexto, no puedes crear la respuesta con otra fuente de informacion no descrita aqui.\n",
        "\n",
        "\n",
        " Contexto:  Los Modelos de Lenguaje de Gran Tamaño (LLMs) son los últimos modelos\n",
        " utilizados en PLN. Su rendimiento superior a los modelos más pequeños los ha\n",
        " hecho increíblemente útiles para los desarrolladores que construyen aplicaciones\n",
        " habilitadas para PLN. Estos modelos se pueden acceder a través de la biblioteca\n",
        " `transformers` de Hugging Face, a través de OpenAI usando la biblioteca `openai`\n",
        " y a través de Cohere usando la biblioteca `cohere`.\n",
        "\n",
        " Question: {query}\n",
        "\n",
        " Answer:\n",
        "\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "cf1aSnhzV6Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PROMPT STRUCTURE O ESTRUCTURA DEL PROMPT**\n",
        "\n",
        "Aunque no parezca los prompts estan compuestos por varias partes.\n",
        "Estas partes nos permiten crear prompts mas claras y efectivas que nos pueden llevar mas facil a nuestros resultados esperados.\n",
        "Asi las cosas, un buen prompt tendrá por lo menos los siguientes elementos.\n",
        "\n",
        "**INSTRUCIONES**\n",
        "\n",
        "Las instruciones son indicaciones que le damos al modelo para que se comporte de una forma en especifico. Esto permite delimitar desde el principio que puede y que no puede hacer el modelo en nustra app.\n",
        "\n",
        "**CONTEXTO**\n",
        "\n",
        "El contexto es informacion base que porporcionamos al modelo para que genere nustras respuestas desde ese punto, puede ser archivos de texto, simplemente texto, o podemos proporcionar ejemplos para que el modelo aprenda de estos como debe dar sus respuestas.\n",
        "\n",
        "**PREGUNTA**\n",
        "\n",
        "Es la consulta o la accion que ordenamos al modelo. La pregunta la pasamos por medio de una variable.\n",
        "\n",
        "\n",
        "Para entender de mejor forma todo lo que aqui se ha dicho, por favor mirar el prompt de arriba."
      ],
      "metadata": {
        "id": "irLLgvL5g8QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template,\n",
        "    #validate_template=False,\n",
        ")"
      ],
      "metadata": {
        "id": "rAr5g1gpVnvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como template pasamos la variable template que contiene nuestro texto prompt, y como input_variables pasamos \"query\" que corresponde a la pregunta.\n",
        "\n",
        "Como se puede observar hay una linea de codigo comentada, aqui la explicacion.\n",
        "Langchain por defecto valida o verifica que las inputs_variables sean las mismas contenidas en la variable template, asi las cosas podemos establecer este parametro como false para que langchain no tenga esta situacion en cuenta. Aqui la use para ver como funcionaba."
      ],
      "metadata": {
        "id": "dVWu8iIcjuGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.format(query=\"esta es la pregunta\")"
      ],
      "metadata": {
        "id": "xRgjRyd3fYtI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "795de187-f8c1-4d03-f13a-ff14f136be41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n Basado en el Contexto que te proporcione responde la pregunta o realiza la solicitud que te hago.\\n Si no sabes la repuesta a la pregunta responde \"Disculpa, no se la respuesta\".\\n La unica informacion de la que podras extraer o crear la respuesta es de la informacion que te\\n proporciono en el Contexto, no puedes crear la respuesta con otra fuente de informacion no descrita aqui.\\n\\n\\n Contexto:  Los Modelos de Lenguaje de Gran Tamaño (LLMs) son los últimos modelos \\n utilizados en PLN. Su rendimiento superior a los modelos más pequeños los ha \\n hecho increíblemente útiles para los desarrolladores que construyen aplicaciones \\n habilitadas para PLN. Estos modelos se pueden acceder a través de la biblioteca \\n `transformers` de Hugging Face, a través de OpenAI usando la biblioteca `openai` \\n y a través de Cohere usando la biblioteca `cohere`.\\n\\n Question: esta es la pregunta\\n\\n Answer: \\n \\n '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formateo el prompt para ver donde establecia la pregunta."
      ],
      "metadata": {
        "id": "pkxDV-fSlEmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CHAIN SECTION**\n",
        "\n"
      ],
      "metadata": {
        "id": "kud3f1y-Zfpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(\n",
        "    prompt = prompt,\n",
        "    llm = llm,\n",
        "    )"
      ],
      "metadata": {
        "id": "PUP4mPg6W6vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"A travez de que bibliotecas se puede acceder a los transformers?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "e9ZetzEkX4V0",
        "outputId": "2fb241e4-28b7-42d6-e605-616ba11720ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Los Modelos de Lenguaje de Gran Tamaño (LLMs) se pueden acceder a través de la biblioteca `transformers` de Hugging Face, a través de OpenAI usando la biblioteca `openai` y a través de Cohere usando la biblioteca `cohere`.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}