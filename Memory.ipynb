{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwQaKk1u83yMp3srqh4/kd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brayan0404/Aprende_Langchain/blob/main/Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MEMORY**\n",
        "\n",
        "Este documento contiene toda la informacion **basica** relevante, a consideracion, acerca de la memoria en Langchain. Estaremos viendo los distintos tipos de memoria que exinten en langchain.\n",
        "\n",
        "# **Que es la memoria en langchain**\n",
        "\n",
        "La idea detras de la memoria en langchain es simple, y su funcion es la de todo tipo de memoria, guardar informacion.\n",
        "Por defecto muchos de los componentes, por miedo a decir todos, son stateless, es decir, sin memoria, esto implica que cuando estas chateando o haciendo completaciones desde un llm estos no tengan un registro de los mensajes anteriores, asi por ejemplo si despues de varios mensajes intercambiados con el llm le preguntas cual fue el primer mensaje que le enviaste, o le haces alguna preunta que tome informacion sobre los mensajes distintos al ultimos el modelo te dirá que no sabe de lo que estas hablando. He aqui donde entra la memoria en juego, lo que viene a resolver es el problema de la perdida de informacion entre interacciones, asi con la memoria el modelo guarda un registro de los mensajes anteriores, y en caso de pregunta sobre un mensaje antiguo este sabra de que le hablas exactamente y podrá brindarte una respuesta basada en el historial de mensajes."
      ],
      "metadata": {
        "id": "wx8odCmQ8RpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ISTALACIONES, CONFIGURACIONES API, IMPORTACIONES**\n",
        "\n",
        "Para este punto cabe hacer una introducion acerca de los paquetes que estamos importando.\n",
        "\n",
        "**ConversationChain**\n",
        "\n",
        "Importamos ConversationChain que es un tipo de chain muy parecido al LLMChain con la diferencia que el primero tiene memoria por defecto, y el segundo no. Al igual que al LLMChain le pasamos un parametro llm y podemos pasar tambien un prompt, y digo podemos, porque por defecto esté trae uno.\n",
        "\n",
        "**PromptTemplate**\n",
        "\n",
        "Inportamos PromptTemplate porque vamos a estar creando un prompt perzonaliza (con nuestras propias instruciones y ejemplos si queremos) para sustituir al que por defecto trae el ConversationChain.\n",
        "\n",
        "### **langchain.memory**\n",
        "\n",
        "Desde el modulo memory de langchain estaremos importando distintos tipos de memoria que funcionan distinto y cada una tiene sus ventajas y desventajas."
      ],
      "metadata": {
        "id": "1mc_JosQCAl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNsHPn8ugeoh"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory, ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "p4ZPL6ALgyHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"coloca_tu_clave_de_openai_api_key_aqui\""
      ],
      "metadata": {
        "id": "Yon8W2Akh63Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(\n",
        "    openai_api_key=\"OPENAI_API_KEY\",\n",
        "    model_name= \"text-davinci-003\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "07CdbBQMh81R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.0)"
      ],
      "metadata": {
        "id": "8L8AOcees4VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONVERSATION CHAIN Y LOS DIFERENTES TIPOS DE MEMORIA**\n",
        "\n",
        "## **EL PROBLEMA DE LA MEMORIA EN LANGCHAIN**\n",
        "\n",
        "El problema al que me refiero es el de la velocidad y la cantidad de tokens influidos por la memoria, a continuacion explico de que van.\n",
        "\n",
        "El template del conversation-chain-memoria es el siguiente:\n",
        "\n",
        "\"\"\"\n",
        "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "Human: {input}\n",
        "AI: \"\"\"\n",
        "\n",
        "Ahora, todos los mensajes se van guardando en la variable history, y que secede?, pues que si los mensajes se guarda en history el tamaño del template va creciendo, y ya sabemos que para cada consulta que hacemos al modelo le pasamos la informacion que tenemos en nuestro template, y como nuestro template ahora es mas grande debido a los mensajes guardados en la history entonces el tiempo de procesamiento será mayor, y como hay mas caracteres tambien será mayor el numero de tokens gastados. Asi a la hora de elegir el tipo de memoria debemos tenee en cuenta al rededos de su funcionamiento estas dos variables."
      ],
      "metadata": {
        "id": "6l0mpEApEZtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationBufferMemory()**\n",
        "\n",
        "Este es un tipo de memoria en langchain, y es quizas el mas simple de todos. Cuando el usuario está conversando con el llm este envia al llm texto como input y recibe texto como output, la funcion de este tipo de memoria es guardar todos los inputs y outputs que se han realizado en la conversation en una variable.\n",
        "\n",
        "### **PROS**\n",
        "Guardar toda la coneversation le da la capacidad al llm de que cualquier interacion pasada pueda ser referenciada en el presente. Asi si en el mensaje uno le preguntaste sobre que son los llms y en el mensaje 10 le preguntas que si lo que dijo en el mensaje uno es relevante entonces sabra a que mensaje te estas refiriendo y te podra responder.\n",
        "\n",
        "### **CONS**\n",
        "Entre mas larga sea la conversation que se tenga que guardar en la memoria el modelo tardará mas en dar una respuesta, y tambien, sera consumidos mas tokens, lo que se traduce en mas dinero gastado, ademas, los llms tienen un limite de tokens, que en openai son de 4096, quiere decir que a partir de aqui el modelo sera insuficiente para guardar mas informacion.\n",
        "\n",
        "\n",
        "**RECOMENDACION**\n",
        "\n",
        "Se recomienda usar este tipo de memoria cuando el usuario va a tener no muchas interaciones con el modelo, por ejemplo 10 interaciones, ya que si la conversacion es sostenida en el tiempo incurre en los problemas de la memoria expresados arriba.\n",
        "\n"
      ],
      "metadata": {
        "id": "IDEoIocFEqFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ],
      "metadata": {
        "id": "n98Piz_WtEDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationSummaryMemoryMemory()**\n",
        "\n",
        "Este tipo de memoria hace un resumen sobre todas las interaciones que hemos teneido con el modelo, y cada nueva interacion es resumida e incluida en el resumen. Recibe como parametro el llm ya que este se encarga de hacer el resumen toda la history.\n",
        "\n",
        "### **PROS**\n",
        "GRacias a que el modelo resume la history, esto nos permite que la cantidad de informacion guardada sea menor que en la ConversationBufferMemory, y esto trae como beneficios que el modelo responda mas rapido, y que podamos ingresar mas interaciones que en el primero.\n",
        "\n",
        "### **CONS**\n",
        "\n",
        "No se recomienda usar este tipo de memoria cuando se van a tener conversaiones cortas, en tanto el numero de tokes es alto para conversaiones cortas en comparacion a otros tipos de memoria, ademas, la capacidad para capturar informacion está directamente ligada a la capacidad de este tipo de memoria de hacer resumenes, es decir, quizas se omita informacion relevante.\n",
        "\n",
        "\n",
        "**RECOMENDACION**\n",
        "\n",
        "Se recomienda usar este tipo de memoria para cuando se esperen tener conversasiones largas, ya que aunque al principo el numero de token usados es bastante alto, con el tiempo tiende a parar en crecimiento."
      ],
      "metadata": {
        "id": "fDRVpM59aTaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationSummaryMemory(llm=llm)\n",
        ")"
      ],
      "metadata": {
        "id": "hXvtSgyMB_9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationBufferWindowMemory(k=n)**\n",
        "\n",
        "Este tipo de memoria hace una lista de las ultimas k interaciones, donde k es un numero que corresponde al numero de interaciones que queremos guardar, asi si colocamos a k=2 quiere decir que se guardaran solo las dos ultimas dos interaciones que tuvimos con el modelo.\n",
        "\n",
        "### **PROS**\n",
        "Las ventajas son claras, al tener limitado el numero de interaciones que podrá recordar estaremos ahorrando tokens y ademas conoceremos el rendimiento del modelo.\n",
        "\n",
        "### **CONS**\n",
        "Puede suceder que las interaciones no son suficientes, por lo que podria llegar a ser frustrante, aunque basta con modificar k para solucionarlo.\n",
        "\n",
        "\n",
        "**RECOMENDACION**\n",
        "Se recomienda ir cambiando el numero de interaciones segun necesidad.\n"
      ],
      "metadata": {
        "id": "ulhEjksglmKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferWindowMemory(k=2)\n",
        ")"
      ],
      "metadata": {
        "id": "S5iUtixsEDCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation(\"Como me llamo?\")"
      ],
      "metadata": {
        "id": "RljsxNG2tfwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code shows the conversation history"
      ],
      "metadata": {
        "id": "Ci5eDk9freH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.memory.buffer)"
      ],
      "metadata": {
        "id": "sQ_iotJSCabf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estos son algunos de los tipos de memoria en langchain, hay otros que quedan por cubrir pero en general son similares."
      ],
      "metadata": {
        "id": "O0xEieRXrcmR"
      }
    }
  ]
}