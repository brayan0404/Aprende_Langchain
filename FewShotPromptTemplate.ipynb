{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO778sc6N61EJSJedD30xlt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brayan0404/Aprende_Langchain/blob/main/FewShotPromptTemplate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FewShotTemplate**\n",
        "\n",
        "Esta es una tecnica para pasar de forma estructurada ejemplos al modelo para que aprenda como debe responder a las preguntas o consultas del usuario. Bien es cierto que los ejemplos podriamos pasarlos con un template normal, pero de esta forma lo esctructuramos mucho mejor."
      ],
      "metadata": {
        "id": "VcfHrHHv6CNF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsc0lOjyp32a"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "9JtYu9TkqhMI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"coloca_tu_open_ai_api_key_aqui\""
      ],
      "metadata": {
        "id": "VUlKiwBMqpb-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(\n",
        "    openai_api_key=\"OPENAI_API_KEY\",\n",
        "    model_name = \"text-davinci-003\",\n",
        "    )"
      ],
      "metadata": {
        "id": "vCzRE5KoIstL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.1)"
      ],
      "metadata": {
        "id": "OUrewi7gIwam"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EJEMPLOS**\n",
        "\n",
        "Para dar los ejemplos pasamos una lista de diccionarios que el modelo mas adelante va a analizar y aprender de ellos."
      ],
      "metadata": {
        "id": "Hjqyiw7V63XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the meaning of life?\",\n",
        "        \"answer\": \"42\"\n",
        "    }, {\n",
        "        \"query\": \"What is the weather like today?\",\n",
        "        \"answer\": \"Cloudy with a chance of memes.\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite movie?\",\n",
        "        \"answer\": \"Terminator\"\n",
        "    }, {\n",
        "        \"query\": \"Who is your best friend?\",\n",
        "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
        "    }, {\n",
        "        \"query\": \"What should I do today?\",\n",
        "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "vz_ySrPuOJkw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "j2XWPeg-O6uV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "K1xcJFWoJ31Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PREFIX**\n",
        "\n",
        "Son las instruciones que damos al modelo, aqui delimitamos la forma en la que queremos que se comporte"
      ],
      "metadata": {
        "id": "RIwQYlRN7K6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"\"\"\n",
        "  Eres un asistente que ayuda a los usuarios respondiendo sus preguntas de forma sarcastica.\n",
        "  A continuacion te daré una serie de ejemplos de una conversasion de usuarios y un modelo\n",
        "  de llm con respuestas sarcasticas. Usaras su mismo estilo de respuestas.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UVGAI5MLPrmm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SUFFIX**\n",
        "\n",
        "Hace referencia al cuerpo del template, la entrada del usuario y la salida de la inteligencia artificial u modelo"
      ],
      "metadata": {
        "id": "OH8y8sFz7U1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "suffix = \"\"\"\n",
        "  User: {query}\n",
        "  AI: \"\"\""
      ],
      "metadata": {
        "id": "rSNAAE6rP0_Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"/n/n\"\n",
        ")"
      ],
      "metadata": {
        "id": "Ey4Q3NDZQG9j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt_template.format(query=\"Estoy sorprendido con esta silla, el plastico es resistente\")"
      ],
      "metadata": {
        "id": "Yor-E1qWR0ir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "8c66f594-3d7c-4701-8a19-5fec4a99c197"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n  Eres un asistente que ayuda a los usuarios respondiendo sus preguntas de forma sarcastica.\\n  A continuacion te daré una serie de ejemplos de una conversasion de usuarios y un modelo \\n  de llm con respuestas sarcasticas. Usaras su mismo estilo de respuestas.\\n/n/n\\nUser: How are you?\\nAI: I can't complain but sometimes I still do.\\n/n/n\\nUser: What time is it?\\nAI: It's time to get a watch.\\n/n/n\\nUser: What is the meaning of life?\\nAI: 42\\n/n/n\\nUser: What is the weather like today?\\nAI: Cloudy with a chance of memes.\\n/n/n\\nUser: What is your favorite movie?\\nAI: Terminator\\n/n/n\\nUser: Who is your best friend?\\nAI: Siri. We have spirited debates about the meaning of life.\\n/n/n\\nUser: What should I do today?\\nAI: Stop talking to chatbots on the internet and go outside.\\n/n/n\\n  User: Estoy sorprendido con esta silla, el plastico es resistente\\n  AI: \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(\n",
        "    prompt=few_shot_prompt_template,\n",
        "    llm=llm,\n",
        ")"
      ],
      "metadata": {
        "id": "EiFg6Ed3KTUa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Amo este producto, no puedo pedir mas\")"
      ],
      "metadata": {
        "id": "p4jybYLVKtO2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5512bf94-8a31-48ea-aa90-f38612f3e87e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Parece que estás satisfecho, ¿por qué no compartes tu experiencia con el mundo?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}