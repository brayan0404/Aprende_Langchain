{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNVkOmCGTBQ8/cYQCyUZia",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brayan0404/Aprende_Langchain/blob/main/LLMChain_PromptTemplate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBJETIVO**\n",
        "\n",
        "*El objetivo de este documento es aprender los basicos de como configurar prompts por medio del metodo encargado de dicha tarea, el promptTemplate method, y los basicos de LLMChain method, para unir el prompt formateado y hacer la respectiva consulta.*\n",
        "游봃\n",
        "\n",
        "\n",
        "# **CONECCION POR MEDIO DE CHAINS, CREACION DE PROMPTS POR MEDIO DE PROMPTSTEMPLATES, Y CONSULTAS A LLMS**\n",
        "\n",
        "# CHAINS\n",
        "En los modulos que siguen se estar치 profundizando sobre que son los chains en langchain, por ahora podemos quedarnos con la idea de que los chains es la forma que tenemos de conectar dos o mas modulos en langchain para crear aplicaciones mas robustas, completas y complejas. Por ejemplo por medio de las chains podemos conectar el modulo de los prompts con el de los llms, y de esta forma podemos crear una app que cree prompts ya configurados con un promptTemplate y pasar esos prompts creados al llm para hacer la respectiva consulta o tarea.\n",
        "\n",
        "# PROMPTS Y PROMPtTEMPLATES\n",
        "Los prompts son cadenas de texto que pasamos a los llms para hacer las consultas, si le vamos a preguntar alggo a chatgpt escribimos nuestra consulta, eso es un prompt, ahora, un promtTemplate, es la forma que tenemos en langchaing de configurar los prompts, es decir, supongamos que queremos crear una aplicacion y esa aplicacion es para ayudar a personas con poca experticia en la cocina, la persona le pregunta a la aplicacion como se hace el arroz, o como se hace la carne, y asi seguidamente, entonces si el usuario pregunta a la semana 5 veces como se hace tal comida seria algo tedioso que siempre este ingresando la frase \"como se hace\", donde seria mas eficiente si le pasa el nombre del plato o la preparacion que quiere y listo, es decir, yuca, platano, y el modelo por dentro tome lo que ingreso, lo una todo y haga la consulta, asi, \"como se hace la (yuca)\", esto es lo que nos ahorran los promptTemplates.\n",
        "\n",
        "# LLMs Y CONSULTAS\n",
        "Los llms no son mas que los modelos de lenguaje. Langchain tiene soporte para muchos, ellos son los actores inteligentes, en este entorno son accedidos por medio de una api aki.\n",
        "\n",
        "\n",
        "# **FUNCIONAMIENTO DEL LLMCHAIN DEL PRESENTE DOCUMENTO**\n",
        "\n",
        "LLMChain es el nombre que se le da a la forma de unir los modulos de llms con promps, en el presente caso, el llm con el promptTemlate.\n",
        "Lo que hace el codigo de abajo en pocas palabras es consfigurar un prompt por medio del PromptTemplate metodo, y hacer la respectiva consulta al llm pero primero haciendo la coneccion entre el promp y el llm por medio del metodo LLMChain, y por ultimo ejecutarlo por medio del metodo run.\n",
        "Cada paso del programa est치 explicado en la respectiva celda.\n",
        "\n"
      ],
      "metadata": {
        "id": "UWMB61J3PMoD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNOgVTRuBsng"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI; from langchain.prompts import PromptTemplate; from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "8bXJynyIBySN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importo Openai desde los llms de langchain porque estare usando su modelo, importo desde el modulo de prompts de langchain el metodo PromptTemplate el cual estare usando para crear un prompt template, y por ultimo importo el metodo LLMChain del modulo chains de langchain porque por medio de este voy a estar tomando el prompt que cree con el promptTemplate y lo pasare al llm que cree con el llm de openai y luego lo ejecutare."
      ],
      "metadata": {
        "id": "-yvPkdvAFiBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(input_variables=[\"product\"], template=\"como hacer un o una {product}\",)"
      ],
      "metadata": {
        "id": "RRW16bHnC5zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.format(product=\"productos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nwTc9kepj0M4",
        "outputId": "ece092b8-6ab9-4e48-ed9c-3efde70f8271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'como hacer un o una productos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta linea de codigo creo una variable llamada prompt la cual usa el metodo PromptTemplate, el cual, crea los prompts personalizados, toma por un lado la variable inputs_variables que contiene el valor que el usuario estara ingresando, y la variable template, la cual contiene el template o texto base al cual se le va a incorporar lo que ingresa el usuario para auto completar la sentencia."
      ],
      "metadata": {
        "id": "h2ork_FEGnWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"coloca_tu_clave_de_openai_api_key_aqui\""
      ],
      "metadata": {
        "id": "mE2-imoIDqLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuro mi api de openai para despues poder conectarme a sus modelos"
      ],
      "metadata": {
        "id": "4xNoGnpIHb9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(openai_api_key=\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "VaHPyHTMDyWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Me conecto a openai por medio de mi api"
      ],
      "metadata": {
        "id": "XPR0Zk5NHmUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "PKNXNtggEDko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "le asigno la correspondiente temperatura para ajustar la libertad/conservatividad de las respuestas."
      ],
      "metadata": {
        "id": "E8ZszaoNHugk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "Dk7DdDQ0EoVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta parte creo una variable chain la cual contiene el metodo LLMChain, lo que hace este metodo es conectar el llm con el propmt, es decir, que toma el propmt que he configurado y creado con el PromptTemplate, y luego lo pasa al llm para hacer la respectiva consulta. De esta forma nos evitamos por un lado tener que escribir una y otra una sentencia repetitiva, que podria ser convertida a un prompt y no tener que pasarla al llm directamente, y por el otro, mos evitamos tener que formatear el texto con el template, pues desde dentro el metodo LLMChain lo formatea.                                                            Ahora bien, el metodo LLMChain tiene dos varibles, por un lado la variable prompt, a la cual le pasamos el prompt que creamos con el meotodo PromptTemplate, y la cual tiene el mismo nombre, es decir, prompt, y la otra variable es llm, a la cual le pasamos la variable llm, la cual corresponde a nuestro llm conectado y configurado."
      ],
      "metadata": {
        "id": "62CjeAhtH9e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"papa frita\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "mpfoHf5uFAcQ",
        "outputId": "d68e0cf8-403a-4a09-8e80-657d432a2032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'?\\n\\nPara hacer una papa frita, primero lava y pela la papa. Luego, c칩rtala en rodajas finas con una mandolina o un cuchillo. Una vez cortadas, coloca las rodajas en un recipiente con agua fr칤a y sal y deja que reposen durante 15 minutos. Esto los ayudar치 a convertirse en crujientes.\\nDespu칠s, s치calas del agua y s칠calas. Calienta aceite a fuego medio-alto hasta que alcance los 175 grados cent칤grados (si no tienes un term칩metro para medir, agrega una peque침a porci칩n de papa al aceite y revisa si comienza a fre칤rse de inmediato).\\n\\nAgrega las rodajas de papa al aceite y fr칤elas hasta que est칠n doradas, unos 4-5 minutos. Usa una espumadera para removerlas de vez en cuando.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por ultimo con esta linea le decimos a la coneccion que hicimos, que con el metodo run, corra el texto que le ingresamos dentro. EL texto que ingresamos dentro del metodo run corresponde a la variable para completar nuestra sentencia definida como prompt. Esta linea hace el llamado al llm para que de la respuesta."
      ],
      "metadata": {
        "id": "1tNji6FrKICT"
      }
    }
  ]
}